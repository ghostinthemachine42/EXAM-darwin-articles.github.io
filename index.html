 <!DOCTYPE html>
<html>
<head>
<title>EXAM: The Exam Answerability Metric</title>
</head>
<body>

<h1>EXAM: The Exam Answerability Metric for Evaluating Retrieve-and-Generate Systems</h1>

<h2>Abstract</h2>
We provide an automatic evaluation measure for systems that retrieve-and-generate comprehensive information for topical information needs. 
Traditional evaluation in information retrieval relies on manual relevance assessments that are not tolerant to minor changes in language, and hence are  
not applicable to information that is derived with methods such as natural language generation. To alleviate this, we offer EXAM, an evaluation paradigm 
that uses held-out exam questions and an automated question-answering system to evaluate how well generated responses  can answer follow-up  questions---without 
knowing the exam questions in advance.

<br><br>

<h2>Appendix</h2>

<h3>Manual Answerability Analysis</h3>
<p>In our work, we briefly analyze four articles all generated based on the topic/query "Darwinâ€™s Theory of Evolution". 
	We expand upon the analysis in the following link:
</p>
<a href="./4_4_3_q_analysis.html">Manual Answerability Analysis</a>
<br>
<h3>Manual Question Analysis</h3>
<p>In our work, we examine the results of scoring the sixteen particpant systems using ten manually-created, natural-sounding questions. We provide
	some additional detail and the ten questions in the following link:</p>
<a href="./4_5_manual_question_analysis.html">Manual Question Analysis</a>

</body>
</html> 
